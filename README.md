# ğŸ§  Neural Network Modifications & Hyperparameter Experiments  

## ğŸ“œ Overview  
This project involves modifying a neural network's **hyperparameters, activation functions, cost functions, and regularization methods** to improve training performance and generalization. Additionally, we conduct **experiments** to analyze the impact of different configurations on model accuracy.  

ğŸ“Œ **Tasks**:  
- Modify the network to support **customizable hyperparameters**.  
- Implement additional **cost functions, activation functions, and regularization methods**.  
- Add **dropout** for regularization.  
- Conduct **experiments** on the **Iris dataset** using different configurations.  

ğŸ“Œ **Programming Language**: `Python 3`  
ğŸ“Œ **Libraries Used**: `NumPy`, `pandas`, `Jupyter Notebook`  

## ğŸš€ 1ï¸âƒ£ Network Code Modifications  

### **Added Hyperparameters**  
- **Cost Functions**: Quadratic, CrossEntropy, LogLikelihood  
- **Activation Functions**: Sigmoid, Tanh, ReLU, LeakyReLU, Softmax  
- **Regularization**: L1, L2  
- **Dropout Rate**  

### **Modified Functions**  
- `set_model_parameters()` â†’ Supports **custom cost & activation functions**.  
- `feedforward()` â†’ Implements **dropout for hidden layers**.  
- `backprop()` â†’ Supports **L1/L2 regularization & dropout in weight updates**.  
- `update_mini_batch()` â†’ Incorporates **new regularization methods**.  
- `total_cost()` â†’ Adjusted for **regularization impact on cost function**.  

ğŸ“Œ **Dropout Implementation**:  
- Applied **only during training** (not evaluation).  
- Uses a **binary mask** to randomly drop units.  
- Ensures **scaling to maintain expected activations**.  

## ğŸ¯ 2ï¸âƒ£ Experimental Setup  

### **Dataset**  
- **Training Set**: `iris-train-2.csv`  
- **Test Set**: `iris-test-2.csv`  
- **Pretrained Models**: `iris-423.dat`, `iris4-20-7-3.dat`  

### **Experimental Parameters**  
- **Epochs**: 30  
- **Mini-batch Size**: 8  
- **Learning Rate (Î·)**: 0.1  
- **Regularization & Dropout**: Various configurations tested.  

ğŸ“Œ **Key Observations**:  
- **LeakyReLU & ReLU** show improvement over **Sigmoid/Tanh**.  
- **Regularization (L1/L2)** reduces **overfitting**.  
- **Dropout (0.3)** helps generalization but **affects training stability**.  

## ğŸ“Œ Summary  
âœ… Added support for multiple cost & activation functions.  

âœ… Implemented L1/L2 regularization & dropout.  

âœ… Modified key functions for new hyperparameters.  

âœ… Conducted extensive experiments on the Iris dataset.  
